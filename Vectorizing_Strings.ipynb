{
 "metadata": {
  "name": "",
  "signature": "sha256:0cb2ccf7626d5520d9217292ac07d04212d407055e64b6f26e507d22b361b05e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have learnt now how to tokenize strings, pandas foo and many other things.  \n",
      "In this session you will learn the following things:\n",
      "\n",
      "* Learn how to convert text into vectors.\n",
      "\n",
      "and some additional concepts as we go (bonus!!!)\n",
      "\n",
      "So lets start with a corpus that Dave has put together.\n",
      "\n",
      "Fetch the data. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why should we vectorize strings/text? \n",
      "\n",
      "* Machine learning algorithms are powerful in understanding and processing numeric data. \n",
      "* Perform machine learning we need to transform strings to numeric vectors.\n",
      "\n",
      "![alt text](data/supervised_scikit_learn.png \"Why Vectorization\")\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "deal_corpus = requests.get('https://raw.github.com/dlemphers-rmn/mlclass/master/data/deals.txt').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deal_corpus = deal_corpus.split('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So lets check how many deal titles we have in our corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(deal_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For any Machine Learning to be performed on textual data specially it is desirable to convert it into numeric vectors. \n",
      "Now how can we convert text data into numbers? \n",
      "\n",
      "* Generating a list of all the unique words in the corpus and then checking presence of each of the words in the corpus\n",
      "\n",
      "This is the NLP jargon is known as [Bag of Words or BOW approach](http://en.wikipedia.org/wiki/Bag-of-words_model)\n",
      "\n",
      "For example:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example_documents = ('Online Fingerstyle Guitar Lessons',\n",
      "'Online Country Guitar Lessons',\n",
      "'Clean Up Your Credit Report',\n",
      "'Learn Double Bass Online',\n",
      "'Folica 25% Off Sitewide')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.feature_extraction.text\n",
      "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print count_vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_vectorizer.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint  \n",
      "pprint.pprint(count_vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert to a vector matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vector_matrix = count_vectorizer.transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vector_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vector_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This type of matrix is called as Term-Document-Matrix\n",
      "\n",
      "![alt text](data/dtm.png \"Why Vectorization\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vanilla count vectorizers are good but they are not powerful enough but there are following issues:\n",
      "\n",
      "* it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms.\n",
      "* we want to emphasize on un(common) terms as a means of describing each vector in a powerful way\n",
      "\n",
      "Solution\n",
      "========\n",
      "\n",
      "Term-frequency inverse document frequency ([tf\u2013idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
      "\n",
      "What tf-idf gives is how important is a word to a document in a collection, and that\u2019s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn\u2019t 10 times more important than it, that\u2019s why tf-idf uses the logarithmic scale to do that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![TFIDF Explained](http://archimedes.fas.harvard.edu/presentations/2002-03-09/img13.gif)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tfidf_vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Refer to [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on tfidf vectorizer for more details."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vectorizer.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidf_vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidfvector_matrix = tfidf_vectorizer.transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tfidfvector_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidfvector_matrix.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidf_vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some tricks\n",
      "===========\n",
      "\n",
      "* we can use n-grams when trying to do tfidf transformation\n",
      "\n",
      "ngrams are explained by Dave in his session"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 4))\n",
      "ngram_tfidf_matrix = ngram_tfidf.fit_transform(example_documents)\n",
      "pprint.pprint(ngram_tfidf.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* we can remove stop words\n",
      "\n",
      "Basically stopwords are very-very common words in the english language"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.corpus\n",
      "stopwords_english = nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(stopwords_english[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For corpus which is typically short we can use character n-grams which are at a character level.\n",
      "\n",
      "This will result in a dense vector representation but helps to extract maximum juice out of the short corpus.\n",
      "\n",
      "*extra: you can use a different tokenizer (ref: Dave's sessions)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 7),analyzer='char')\n",
      "ngram_tfidf_matrix = ngram_tfidf.fit_transform(example_documents)\n",
      "pprint.pprint(ngram_tfidf.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf_deal_corpus = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 7),analyzer='char',stop_words=stopwords_english)\n",
      "ngram_tfidf_matrix_deal_corpus = ngram_tfidf.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(ngram_tfidf_matrix_deal_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These vector matrices can be used as datapoints to perform: \n",
      "\n",
      "- clustering (unsupervised learning)\n",
      "- classification (supervised learning)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What if My Data is a JSON object? Can I convert into a Machine Learning friendly format? \n",
      "\n",
      "Yes!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case scikit-learn provides a Dictionary Vectorizer to convert your Dictionary(Python) into a NumPy representation which is ML friendly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cities = ['Austin, TX', 'London, UK', 'Berlin, DE', 'Vannes, FR', 'Paris, FR', 'Amsterdam, NL']\n",
      "all_weather = []\n",
      "for city in cities:\n",
      "    weather_request = requests.get('http://api.openweathermap.org/data/2.5/weather?q={},&units=metric'.format(city))\n",
      "    if weather_request.status_code == 200:\n",
      "        weather_data = json.loads(weather_request.text)['main']\n",
      "        weather_data['city'] = city\n",
      "        all_weather.append(weather_data)\n",
      "    else:\n",
      "        print \"Faild to get {}\".format(city)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.feature_extraction\n",
      "v = sklearn.feature_extraction.DictVectorizer(sparse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_weather"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[{'city': 'Austin, TX',\n",
        "  u'humidity': 49,\n",
        "  u'pressure': 1007,\n",
        "  u'temp': 29.15,\n",
        "  u'temp_max': 31.11,\n",
        "  u'temp_min': 27.22},\n",
        " {'city': 'London, UK',\n",
        "  u'humidity': 87,\n",
        "  u'pressure': 1019,\n",
        "  u'temp': 11.62,\n",
        "  u'temp_max': 13.33,\n",
        "  u'temp_min': 10.56},\n",
        " {'city': 'Berlin, DE',\n",
        "  u'humidity': 87,\n",
        "  u'pressure': 1021,\n",
        "  u'temp': 14.21,\n",
        "  u'temp_max': 15.4,\n",
        "  u'temp_min': 13},\n",
        " {'city': 'Vannes, FR',\n",
        "  u'humidity': 89,\n",
        "  u'pressure': 1018,\n",
        "  u'temp': 11.11,\n",
        "  u'temp_max': 11.11,\n",
        "  u'temp_min': 11.11},\n",
        " {'city': 'Paris, FR',\n",
        "  u'humidity': 67,\n",
        "  u'pressure': 1019,\n",
        "  u'temp': 14.16,\n",
        "  u'temp_max': 16.67,\n",
        "  u'temp_min': 12},\n",
        " {'city': 'Amsterdam, NL',\n",
        "  u'humidity': 87,\n",
        "  u'pressure': 1019,\n",
        "  u'temp': 14.23,\n",
        "  u'temp_max': 16.67,\n",
        "  u'temp_min': 12.78}]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = v.fit_transform(all_weather)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<6x11 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 36 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "array([[  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          4.90000000e+01,   1.00700000e+03,   2.91500000e+01,\n",
        "          3.11100000e+01,   2.72200000e+01],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          8.70000000e+01,   1.01900000e+03,   1.16200000e+01,\n",
        "          1.33300000e+01,   1.05600000e+01],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          8.70000000e+01,   1.02100000e+03,   1.42100000e+01,\n",
        "          1.54000000e+01,   1.30000000e+01],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
        "          8.90000000e+01,   1.01800000e+03,   1.11100000e+01,\n",
        "          1.11100000e+01,   1.11100000e+01],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
        "          6.70000000e+01,   1.01900000e+03,   1.41600000e+01,\n",
        "          1.66700000e+01,   1.20000000e+01],\n",
        "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
        "          8.70000000e+01,   1.01900000e+03,   1.42300000e+01,\n",
        "          1.66700000e+01,   1.27800000e+01]])"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.feature_names_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['city=Amsterdam, NL',\n",
        " 'city=Austin, TX',\n",
        " 'city=Berlin, DE',\n",
        " 'city=London, UK',\n",
        " 'city=Paris, FR',\n",
        " 'city=Vannes, FR',\n",
        " u'humidity',\n",
        " u'pressure',\n",
        " u'temp',\n",
        " u'temp_max',\n",
        " u'temp_min']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There you go! Your Data is now converted into ML friendly format."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Excersise (Pandas FOO!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can you convert `all_weather` into a pandas dataframe and sort cities based on `humidity`? \n",
      "\n",
      "Question 2: Once you do this can you name the most most and least humid city. \n",
      "\n",
      "If stuck the execute the following cell. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -r 1-5 solutions/sol2.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "weather_df = pd.DataFrame(weather_data)\n",
      "weather_df = weather_df.sort(['humidity', 'temp'], ascending=False)\n",
      "weather_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "If using all scalar values, you must must pass an index",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-23-9e6bb9d28896>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mweather_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweather_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mweather_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweather_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'humidity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'temprature'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mweather_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    199\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    200\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         return _arrays_to_mgr(arrays, data_names, index, columns,\n\u001b[1;32m--> 323\u001b[1;33m                               dtype=dtype)\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     def _init_ndarray(self, values, index, columns, dtype=None,\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   4461\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4462\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4463\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4464\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4465\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   4500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4502\u001b[1;33m             raise ValueError('If using all scalar values, you must must pass'\n\u001b[0m\u001b[0;32m   4503\u001b[0m                              ' an index')\n\u001b[0;32m   4504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must must pass an index"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weather_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "{'city': 'Amsterdam, NL',\n",
        " u'humidity': 87,\n",
        " u'pressure': 1019,\n",
        " u'temp': 14.23,\n",
        " u'temp_max': 16.67,\n",
        " u'temp_min': 12.78}"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}